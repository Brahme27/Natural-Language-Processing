{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zaynb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "messages=pd.read_csv('smsspamcollection.csv',sep='\\t',names=[\"Label\",\"Messages\"]) \n",
    "type(messages)\n",
    "messages\n",
    "## Data cleaning and Preprocessing \n",
    "import re \n",
    "import nltk \n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import  PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "corpus=[]\n",
    "for i in range(0,len(messages)):\n",
    "    #For removing unnessasary characters\n",
    "    review=re.sub('[^a-zA-Z]',' ',messages['Messages'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    #print(review)\n",
    "    #Applying porter Stemming for list of words\n",
    "    review=[ps.stem(word) for word in review if word not in stopwords.words(\"english\")]\n",
    "    review=\" \".join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying BOW words \n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=100,binary=True)\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 100 words with more frequency(onserve all are unigrams)\n",
    "#cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets apply bag of words with n gram\n",
    "# Applying BOW words \n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=100,binary=True,ngram_range=(1,1))\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv.vocabulary_# here all are unigrams  since range is given as ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets apply bag of words with n gram\n",
    "# Applying BOW words \n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=500,binary=True,ngram_range=(1,2))\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv.vocabulary_ # combination of unigrams and bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets apply bag of words with n gram\n",
    "# Applying BOW words \n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=500,binary=True,ngram_range=(2,2))\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see only bi grams\n",
    "#cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets apply bag of words with n gram\n",
    "# Applying BOW words \n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=500,binary=True,ngram_range=(3,3))\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see all trigrams\n",
    "#cv.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ntry to play with hyperparameters to increase the accuracy of the model\\nby changing the ngram_range (1,1),(1,2),(2,2),(2,3),(3,3),(1,3) ..... and also by changing max features\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "try to play with hyperparameters to increase the accuracy of the model\n",
    "by changing the ngram_range (1,1),(1,2),(2,2),(2,3),(3,3),(1,3) ..... and also by changing max features\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
